# -*- coding: utf-8 -*-
"""no_pad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w3ZLVx2DosI5c0tH5_VJZGWvkiZtaasr
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pickle
from collections import defaultdict
import sys, re
import pandas as pd
import re
from tqdm import tqdm
import matplotlib.pyplot as plt

home_dir = '/content/drive/My Drive/11747/'

# dataset path
train_path = home_dir + 'topicclass/topicclass_train.txt'
valid_path = home_dir + 'topicclass/topicclass_valid.txt'
test_path = home_dir + 'topicclass/topicclass_test.txt'

"""# Data Pre-process"""

def load_data(filename):
  text = []
  labels = []
  length_vec = []
  word_dict = defaultdict(int)
  with open(filename, "r") as f:
      for line in f:
          tag, words = line.strip().split(" ||| ")
          # remove weird words as such @
          s = re.sub(r"[^A-Za-z0-9()!#$%&*+,./:;<=>?\[\\\]{|~}â€“\"\-\'\`]", " ", words)
          text.append(s)
          labels.append(tag)
          s_split = s.split()
          for w in s_split:
              word_dict[w] += 1
          length_vec.append(len(s_split))
      return text, labels, word_dict, length_vec

x_train, y_train, word_dict_train, length_vec_train = load_data(train_path)
print("total train set: ", len(x_train))
print('number of all vocab', len(word_dict_train))

num_classes = len(set(y_train))
label2idx_dict = {}
idx2label = {}
i = 0
for label in set(y_train):
  label2idx_dict[label] = i
  idx2label[i] = label
  i += 1

label2idx_dict

idx2label

def get_word2idx(dic, min_freq=3):
  index_dic = {}
  i = 0
  index_dic['<pad>'] = i # 0 for pad
  i += 1
  index_dic['<unk>'] = i # 1 for unk
  i += 1
  for k,v in dic.items():
    if v >= min_freq:
      index_dic[k] = i
      i += 1
  return index_dic

word2idx_train_orig = get_word2idx(word_dict_train)
print("length of indexed vocab", len(word2idx_train_orig))

n_features = 300

# encode sentence to a list of int without padding
def text_to_vector_no_pad(text):
  # remove weird characters
  s = re.sub(r"[^A-Za-z0-9()!#$%&*+,./:;<=>?\[\\\]{|~}\"\-\'\`]", " ", text)
  sent = s.split()
  ret = [1]*len(sent)
  for i in range(len(sent)):
    wd = sent[i]
    if wd in word2idx_train:
      ret[i] = word2idx_train[wd]
  return ret



def get_feature_no_pad(data):
  feat = []
  for sent in data:
    v = text_to_vector_no_pad(sent)
    feat.append(v)
  return np.array(feat)

feature_train = get_feature_no_pad(x_train)
feature_train

def encode_label(label):
  encode = []
  for l in label:
    if l == 'Media and darama': l = 'Media and drama'
    encode.append(label2idx_dict[l])
  return np.array(encode)

y_train_encode = encode_label(y_train)
y_train_encode

x_valid, y_valid, _, length_vec_valid = load_data(valid_path)
print("total valid set: ", len(x_valid))

feature_valid = get_feature_no_pad(x_valid)
y_valid_encode = encode_label(y_valid)

x_test, y_test_fake, _, _ = load_data(test_path)
print("total test set: ", len(x_test))

feature_test = get_feature_no_pad(x_test)

# save and load data
def save(data, title):
  pickle.dump(data, open( home_dir + title + ".pkl", "wb" ))

def load(title):
  data = pickle.load(open( home_dir + title + ".pkl", "rb" )) 
  return data

def get_word_embedding_matrix(word2idx_dict):
    # randomize embedding
    M = (np.random.rand(len(word2idx_dict), 300)-0.5)/2
    pretrain_used = 0
    with open(home_dir + 'cc.en.300.vec', 'r') as f:
      total_pretrained_vec, dim = f.readline().split()
      print("total pretrained vec:", total_pretrained_vec, "dim:", dim)
      for line in f:
        tokens = line.rstrip().split(' ')
        word = tokens[0]
        if word in word2idx_dict:
          M[word2idx_dict[word]] = np.array(tokens[1:])
          pretrain_used += 1

      print("%s out of %s (%f%%) words has pretrained embedding" % (pretrain_used, len(word2idx_dict), pretrain_used/len(word2idx_dict)) )

      return M

embedding = get_word_embedding_matrix(word2idx_train)
save(embedding, "embedding")

embedding = load("embedding")
embedding.shape

"""# Model Train

### Cite: https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/
"""

import torch   
from torchtext import data    
import torch.nn as nn
import torch.optim as optim
from torchtext.data import Field

#Cuda algorithms
torch.backends.cudnn.deterministic = True

# train dataframe to csv
df_train = pd.DataFrame(list(zip(x_train, y_train_encode)), columns =['text', 'label'])
df_train.to_csv(home_dir+"train_topicclass.csv")

# load data from csv
tokenize = lambda x: x.split()

TEXT = Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True)
LABEL = Field(sequential=False, use_vocab=False)

fields = [(None, None), ('text',TEXT),('label', LABEL)]
train_data=data.TabularDataset(path = home_dir+'train_topicclass.csv',format = 'csv',fields = fields,skip_header = True)

print(vars(train_data.examples[0]))

# valid dataframe to csv
df_valid = pd.DataFrame(list(zip(x_valid, y_valid_encode)), columns =['text', 'label'])
df_valid.to_csv(home_dir+"valid_topicclass.csv")

# load from csv
valid_data=data.TabularDataset(path = home_dir+'valid_topicclass.csv',format = 'csv',fields = fields,skip_header = True)
print(vars(valid_data.examples[0]))

# test dataframe to csv
df_test = pd.DataFrame(list(zip(x_test, [16]*len(x_test))), columns =['text', 'label'])
df_test.to_csv(home_dir+"test_topicclass.csv")

# load from csv
test_data=data.TabularDataset(path = home_dir+'test_topicclass.csv',format = 'csv',fields = fields,skip_header = True)
print(vars(test_data.examples[0]))

# glove embeddings
TEXT.build_vocab(train_data,min_freq=3, vectors = "glove.6B.300d")  
# TEXT.build_vocab(train_data, min_freq=3)  
LABEL.build_vocab(train_data)

# number of unique tokens
print("Size of text vocab:",len(TEXT.vocab))

# number of unique label
print("Size of label:",len(LABEL.vocab))

word2idx_train = TEXT.vocab.stoi

def get_word_embedding_matrix(word2idx_dict):
    # randomize embedding
    M = (np.random.rand(len(word2idx_dict), 300)-0.5)/2
    pretrain_used = 0
    with open(home_dir + 'cc.en.300.vec', 'r') as f:
      total_pretrained_vec, dim = f.readline().split()
      print("total pretrained vec:", total_pretrained_vec, "dim:", dim)
      for line in f:
        tokens = line.rstrip().split(' ')
        word = tokens[0]
        if word in word2idx_dict:
          M[word2idx_dict[word]] = np.array(tokens[1:])
          pretrain_used += 1

      print("%s out of %s (%f) words has pretrained embedding" % (pretrain_used, len(word2idx_dict), pretrain_used/len(word2idx_dict)) )
      return M

embedding = get_word_embedding_matrix(word2idx_train)

save(embedding, "embedding_no_pad_lower")
embedding.shape

# check cuda
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  

#set batch size
BATCH_SIZE = 32

#Load an iterator
train_iterator, valid_iterator = data.BucketIterator.splits(
    (train_data, valid_data), 
    batch_size = BATCH_SIZE,
    sort_key = lambda x: len(x.text),
    sort_within_batch=True,
    device = device)

class classifier(nn.Module):
    
    #define all the layers used in model
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, 
                 bidirectional, dropout):
        
        #Constructor
        super().__init__()          
        
        #embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        #lstm layer
        self.lstm = nn.LSTM(embedding_dim, 
                           hidden_dim, 
                           num_layers=n_layers, 
                           bidirectional=bidirectional, 
                           dropout=dropout,
                           batch_first=True)
        
        #dense layer
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        
        #activation function
        self.act = nn.Sigmoid()
        
    def forward(self, text, text_lengths):
        
        text = torch.transpose(text, 0, 1) # [batch size, sent_length]
        
        embedded = self.embedding(text) # [batch size, sent_len, emb dim]
        
        # packed sequence
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)
        
        packed_output, (hidden, cell) = self.lstm(packed_embedded)
        
        # concat the final forward and backward hidden state
        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim = 1) # [batch size, hid dim * num directions]
        
        dense_outputs = self.fc(hidden)

        # Final activation function
        outputs=self.act(dense_outputs)

        return outputs

#define hyperparameters
n_features = 300
num_classes = 16

size_of_vocab = len(TEXT.vocab)
embedding_dim = n_features
num_output_nodes = num_classes
num_hidden_nodes = 128
num_layers = 2
dropout = 0.3
lr = 0.0005

#instantiate the model
model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, 
                   bidirectional=True, dropout = dropout)
print(model)

#Initialize the pretrained embedding
pretrained_embeddings = TEXT.vocab.vectors
model.embedding.weight.data.copy_(pretrained_embeddings)
print(pretrained_embeddings.shape)


# embedding = load("embedding_no_pad_lower")
# embedding = torch.from_numpy(embedding)
# model.embedding.weight.data.copy_(embedding)
# embedding.shape

#define optimizer and loss
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()

# define metric
def calculate_accuracy(preds, y):
    predict = torch.argmax(preds, dim=-1)
    acc = torch.sum(predict == y).float()/len(predict)
    return acc
    
# push to cuda if available
model = model.to(device)
criterion = criterion.to(device)

def train(model, iterator, optimizer, criterion):
    
    #initialize every epoch 
    epoch_loss = 0
    epoch_acc = 0
    
    #set the model in training phase
    model.train()  
    
    for batch in iterator:
        
        #resets the gradients after every batch
        optimizer.zero_grad()   
        
        #retrieve text and no. of words
        text, text_lengths = batch.text
        
        #convert to 1D tensor
        predictions = model(text, text_lengths).squeeze()  
        
        #compute the loss
        loss = criterion(predictions, batch.label)        
        
        #compute the binary accuracy
        acc = calculate_accuracy(predictions, batch.label)   
        
        #backpropage the loss and compute the gradients
        loss.backward()       
        
        #update the weights
        optimizer.step()      
        
        #loss and accuracy
        epoch_loss += loss.item()  
        epoch_acc += acc.item()    
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def save_model(model, ep):
  torch.save(model.state_dict(), home_dir+"no_pad_ass1_epoch"+str(ep)+".pt")

def evaluate(model, iterator, criterion):
    
    #initialize every epoch
    epoch_loss = 0
    epoch_acc = 0

    #deactivating dropout layers
    model.eval()
    
    #deactivates autograd
    with torch.no_grad():
    
        for batch in iterator:
        
            #retrieve text and no. of words
            text, text_lengths = batch.text
            
            #convert to 1d tensor
            predictions = model(text, text_lengths).squeeze()
            
            #compute loss and accuracy
            loss = criterion(predictions, batch.label)
            acc = calculate_accuracy(predictions, batch.label)
            
            #keep track of loss and accuracy
            epoch_loss += loss.item()
            epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

N_EPOCHS = 10
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
     
    #train the model
    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)
    
    #evaluate the model
    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)
    
    #save the best model
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), home_dir+'no_pad_best_epoch.pt')
    
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')
    save_model(model, epoch)
    print("saving model", epoch)

def model_load(name):
  model =  classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, 
                   bidirectional=True, dropout = dropout)
  model.load_state_dict(torch.load(home_dir+name))
  model.eval()
  model.to(device)
  return model


model = model_load("no_pad_best_epoch.pt")

valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)
print(valid_acc)

#Load an iterator
test_iterator = data.BucketIterator(
    test_data, 
    batch_size = 64,
    sort_key = lambda x: len(x.text),
    sort_within_batch=False,
    device = device,
    train=False, shuffle = None)

TEXT_to_word = TEXT.vocab.itos
word_to_TEXT = TEXT.vocab.stoi

with open(home_dir+"dev_results.txt", "w") as f:
  for test in valid_data:
    t = torch.LongTensor([[word_to_TEXT[w]] for w in test.text]).cuda()
    
    text_lengths = torch.LongTensor([len(test.text)])
    predictions = model(t, text_lengths).squeeze()
    predict = torch.argmax(predictions, dim=-1)
    f.write(idx2label[predict.item()]+"\n")

with open(home_dir+"test_results.txt", "w") as f:
  for test in test_data:
    t = torch.LongTensor([[word_to_TEXT[w]] for w in test.text]).cuda()
    
    text_lengths = torch.LongTensor([len(test.text)])
    predictions = model(t, text_lengths).squeeze()
    predict = torch.argmax(predictions, dim=-1)
    f.write(idx2label[predict.item()]+"\n")

