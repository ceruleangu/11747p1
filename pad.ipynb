{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kojWpZ63-mbu",
        "colab_type": "code",
        "outputId": "a6d3d8d1-d351-474c-c5df-a1af07a1e3ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abd2k6_sFEjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import sys, re\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "home_dir = '/content/drive/My Drive/11747/'\n",
        "\n",
        "# dataset path\n",
        "train_path = home_dir + 'topicclass/topicclass_train.txt'\n",
        "valid_path = home_dir + 'topicclass/topicclass_valid.txt'\n",
        "test_path = home_dir + 'topicclass/topicclass_test.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNpUL_FhIw3u",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4sofqdhKsF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filename):\n",
        "  text = []\n",
        "  labels = []\n",
        "  length_vec = []\n",
        "  word_dict = defaultdict(int)\n",
        "  with open(filename, \"r\") as f:\n",
        "      for line in f:\n",
        "          tag, words = line.strip().split(\" ||| \")\n",
        "          # remove weird words such as @\n",
        "          s = re.sub(r\"[^A-Za-z0-9()!#$%&*+,./:;<=>?\\[\\\\\\]{|~}â€“\\\"\\-\\'\\`]\", \" \", words)\n",
        "          text.append(s)\n",
        "          labels.append(tag)\n",
        "          s_split = s.split()\n",
        "          for w in s_split:\n",
        "              word_dict[w] += 1\n",
        "          length_vec.append(len(s_split))\n",
        "      return text, labels, word_dict, length_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1SbEBkbbHRw",
        "colab_type": "code",
        "outputId": "07d51428-6171-4994-a7ce-a48419906902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_train, y_train, word_dict_train, length_vec_train = load_data(train_path)\n",
        "print(\"total train set: \", len(x_train))\n",
        "print('number of all vocab', len(word_dict_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total train set:  253909\n",
            "number of all vocab 131801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmjJgkSKMvV7",
        "colab_type": "code",
        "outputId": "42eb8dac-4ad4-43c6-84ad-dbd0ee16e3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "plt.hist(length_vec_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.7629e+04, 5.6596e+04, 6.9755e+04, 5.3531e+04, 3.1038e+04,\n",
              "        1.5374e+04, 7.0710e+03, 2.8770e+03, 3.4000e+01, 4.0000e+00]),\n",
              " array([ 5. , 12.1, 19.2, 26.3, 33.4, 40.5, 47.6, 54.7, 61.8, 68.9, 76. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAUmklEQVR4nO3db4xd9X3n8fenOBBKCzbBayGPtWYV\nKxFFyz8LHCWqWlCMDVXMg5QFVfUIWXglnDaRKnVNV1proZHIk9IgpUhWcLGrbCilzWLxJ+6sQ1Xt\nSiYMgfLPQZ4QI48FeIINbIMaFvrdB/c3ye0w47mGmbnX8fslHd3f+Z7fOfd7PeDPnHPPvU5VIUk6\ntf1KvxuQJPWfYSBJMgwkSYaBJAnDQJIELOp3Ax/WeeedVytXrux3G5J00njqqad+UlVLp9t20obB\nypUrGR0d7XcbknTSSPLKTNu8TCRJMgwkSYaBJAnDQJKEYSBJoocwSPKpJM90LW8n+UqSc5OMJDnQ\nHpe0+Ulyd5KxJM8muazrWMNt/oEkw131y5M81/a5O0nm5+VKkqYzaxhU1UtVdUlVXQJcDrwDfAfY\nCuytqlXA3rYOsB5Y1ZbNwD0ASc4FtgFXAlcA2yYDpM25pWu/dXPy6iRJPTnRy0RXAz+qqleADcDO\nVt8JXN/GG4Bd1bEPWJzkfOAaYKSqjlbVMWAEWNe2nV1V+6rzfdq7uo4lSVoAJxoGNwLfbuNlVfVq\nG78GLGvj5cChrn3GW+149fFp6h+QZHOS0SSjExMTJ9i6JGkmPX8COcnpwBeA26Zuq6pKMu//Sk5V\nbQe2A6xevdp/lecErNz6SF+e9+Cd1/XleSWdmBM5M1gP/KCqXm/rr7dLPLTHI61+GFjRtd9Qqx2v\nPjRNXZK0QE4kDG7iF5eIAHYDk3cEDQMPddU3truK1gBvtctJe4C1SZa0N47XAnvatreTrGl3EW3s\nOpYkaQH0dJkoyVnA54H/3FW+E3ggySbgFeCGVn8UuBYYo3Pn0c0AVXU0yR3Ak23e7VV1tI1vBe4D\nzgQea4skaYH0FAZV9VPgE1Nqb9C5u2jq3AK2zHCcHcCOaeqjwEW99CJJmnt+AlmSZBhIkgwDSRKG\ngSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk\nDANJEoaBJAnDQJJEj2GQZHGSB5P8MMn+JJ9Jcm6SkSQH2uOSNjdJ7k4yluTZJJd1HWe4zT+QZLir\nfnmS59o+dyfJ3L9USdJMej0z+Drw3ar6NHAxsB/YCuytqlXA3rYOsB5Y1ZbNwD0ASc4FtgFXAlcA\n2yYDpM25pWu/dR/tZUmSTsSsYZDkHOA3gXsBqurdqnoT2ADsbNN2Ate38QZgV3XsAxYnOR+4Bhip\nqqNVdQwYAda1bWdX1b6qKmBX17EkSQuglzODC4AJ4C+TPJ3km0nOApZV1attzmvAsjZeDhzq2n+8\n1Y5XH5+m/gFJNicZTTI6MTHRQ+uSpF70EgaLgMuAe6rqUuCn/OKSEADtN/qa+/b+raraXlWrq2r1\n0qVL5/vpJOmU0UsYjAPjVfVEW3+QTji83i7x0B6PtO2HgRVd+w+12vHqQ9PUJUkLZNYwqKrXgENJ\nPtVKVwMvAruByTuChoGH2ng3sLHdVbQGeKtdTtoDrE2ypL1xvBbY07a9nWRNu4toY9exJEkLYFGP\n8/4A+FaS04GXgZvpBMkDSTYBrwA3tLmPAtcCY8A7bS5VdTTJHcCTbd7tVXW0jW8F7gPOBB5riyRp\ngfQUBlX1DLB6mk1XTzO3gC0zHGcHsGOa+ihwUS+9SJLmnp9AliQZBpIkw0CShGEgSaL3u4k0B1Zu\nfaTfLUjStDwzkCQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiT80JnmWT8/aHfwzuv69tzSycYz\nA0mSYSBJMgwkSRgGkiQMA0kShoEkiR7DIMnBJM8leSbJaKudm2QkyYH2uKTVk+TuJGNJnk1yWddx\nhtv8A0mGu+qXt+OPtX0z1y9UkjSzEzkz+O2quqSqVrf1rcDeqloF7G3rAOuBVW3ZDNwDnfAAtgFX\nAlcA2yYDpM25pWu/dR/6FUmSTthHuUy0AdjZxjuB67vqu6pjH7A4yfnANcBIVR2tqmPACLCubTu7\nqvZVVQG7uo4lSVoAvYZBAX+f5Kkkm1ttWVW92savAcvaeDlwqGvf8VY7Xn18mvoHJNmcZDTJ6MTE\nRI+tS5Jm0+vXUXyuqg4n+XfASJIfdm+sqkpSc9/ev1VV24HtAKtXr57355OkU0VPZwZVdbg9HgG+\nQ+ea/+vtEg/t8UibfhhY0bX7UKsdrz40TV2StEBmDYMkZyX59ckxsBZ4HtgNTN4RNAw81Ma7gY3t\nrqI1wFvtctIeYG2SJe2N47XAnrbt7SRr2l1EG7uOJUlaAL1cJloGfKfd7bkI+B9V9d0kTwIPJNkE\nvALc0OY/ClwLjAHvADcDVNXRJHcAT7Z5t1fV0Ta+FbgPOBN4rC2SpAUyaxhU1cvAxdPU3wCunqZe\nwJYZjrUD2DFNfRS4qId+JUnzwE8gS5IMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CS\nhGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjiBMEhyWpKnkzzc1i9I\n8kSSsSR/neT0Vj+jrY+17Su7jnFbq7+U5Jqu+rpWG0uyde5eniSpFydyZvBlYH/X+teAu6rqk8Ax\nYFOrbwKOtfpdbR5JLgRuBH4DWAf8RQuY04BvAOuBC4Gb2lxJ0gLpKQySDAHXAd9s6wGuAh5sU3YC\n17fxhrZO2351m78BuL+qflZVPwbGgCvaMlZVL1fVu8D9ba4kaYH0embw58AfA//a1j8BvFlV77X1\ncWB5Gy8HDgG07W+1+T+vT9lnpvoHJNmcZDTJ6MTERI+tS5JmM2sYJPkd4EhVPbUA/RxXVW2vqtVV\ntXrp0qX9bkeSfmks6mHOZ4EvJLkW+DhwNvB1YHGSRe23/yHgcJt/GFgBjCdZBJwDvNFVn9S9z0x1\nSdICmPXMoKpuq6qhqlpJ5w3g71XV7wGPA19s04aBh9p4d1unbf9eVVWr39juNroAWAV8H3gSWNXu\nTjq9PcfuOXl1kqSe9HJmMJP/Atyf5E+Bp4F7W/1e4K+SjAFH6fzlTlW9kOQB4EXgPWBLVb0PkORL\nwB7gNGBHVb3wEfqSJJ2gEwqDqvoH4B/a+GU6dwJNnfMvwO/OsP9Xga9OU38UePREepEkzR0/gSxJ\nMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwk\nSRgGkiQMA0kShoEkCcNAkoRhIEkCFs02IcnHgX8EzmjzH6yqbUkuAO4HPgE8Bfx+Vb2b5AxgF3A5\n8Abwn6rqYDvWbcAm4H3gD6tqT6uvA74OnAZ8s6runNNXqVPSyq2P9OV5D955XV+eV/ooejkz+Blw\nVVVdDFwCrEuyBvgacFdVfRI4RucvedrjsVa/q80jyYXAjcBvAOuAv0hyWpLTgG8A64ELgZvaXEnS\nApk1DKrjn9vqx9pSwFXAg62+E7i+jTe0ddr2q5Ok1e+vqp9V1Y+BMeCKtoxV1ctV9S6ds40NH/mV\nSZJ61tN7Bu03+GeAI8AI8CPgzap6r00ZB5a38XLgEEDb/hadS0k/r0/ZZ6b6dH1sTjKaZHRiYqKX\n1iVJPegpDKrq/aq6BBii85v8p+e1q5n72F5Vq6tq9dKlS/vRgiT9Ujqhu4mq6k3gceAzwOIkk29A\nDwGH2/gwsAKgbT+HzhvJP69P2WemuiRpgcwaBkmWJlncxmcCnwf20wmFL7Zpw8BDbby7rdO2f6+q\nqtVvTHJGuxNpFfB94ElgVZILkpxO503m3XPx4iRJvZn11lLgfGBnu+vnV4AHqurhJC8C9yf5U+Bp\n4N42/17gr5KMAUfp/OVOVb2Q5AHgReA9YEtVvQ+Q5EvAHjq3lu6oqhfm7BVKkmY1axhU1bPApdPU\nX6bz/sHU+r8AvzvDsb4KfHWa+qPAoz30K0maB34CWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEY\nSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkiR7CIMmKJI8n\neTHJC0m+3OrnJhlJcqA9Lmn1JLk7yViSZ5Nc1nWs4Tb/QJLhrvrlSZ5r+9ydJPPxYiVJ0+vlzOA9\n4I+q6kJgDbAlyYXAVmBvVa0C9rZ1gPXAqrZsBu6BTngA24ArgSuAbZMB0ubc0rXfuo/+0iRJvZo1\nDKrq1ar6QRv/X2A/sBzYAOxs03YC17fxBmBXdewDFic5H7gGGKmqo1V1DBgB1rVtZ1fVvqoqYFfX\nsSRJC+CE3jNIshK4FHgCWFZVr7ZNrwHL2ng5cKhrt/FWO159fJr6dM+/OcloktGJiYkTaV2SdBw9\nh0GSXwP+FvhKVb3dva39Rl9z3NsHVNX2qlpdVauXLl06308nSaeMnsIgycfoBMG3qurvWvn1domH\n9nik1Q8DK7p2H2q149WHpqlLkhZIL3cTBbgX2F9Vf9a1aTcweUfQMPBQV31ju6toDfBWu5y0B1ib\nZEl743gtsKdtezvJmvZcG7uOJUlaAIt6mPNZ4PeB55I802p/AtwJPJBkE/AKcEPb9ihwLTAGvAPc\nDFBVR5PcATzZ5t1eVUfb+FbgPuBM4LG2SJIWyKxhUFX/G5jpvv+rp5lfwJYZjrUD2DFNfRS4aLZe\nJEnzw08gS5J6ukz0S2fl1kf63YIkDRTPDCRJhoEkyTCQJGEYSJIwDCRJGAaSJE7RW0ul+dSvW5cP\n3nldX55Xvxw8M5AkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJoocw\nSLIjyZEkz3fVzk0ykuRAe1zS6klyd5KxJM8muaxrn+E2/0CS4a765Umea/vcnSRz/SIlScfXy5nB\nfcC6KbWtwN6qWgXsbesA64FVbdkM3AOd8AC2AVcCVwDbJgOkzbmla7+pzyVJmmezhkFV/SNwdEp5\nA7CzjXcC13fVd1XHPmBxkvOBa4CRqjpaVceAEWBd23Z2Ve2rqgJ2dR1LkrRAPux7Bsuq6tU2fg1Y\n1sbLgUNd88Zb7Xj18Wnq00qyOcloktGJiYkP2bokaaqP/AZy+42+5qCXXp5re1WtrqrVS5cuXYin\nlKRTwocNg9fbJR7a45FWPwys6Jo31GrHqw9NU5ckLaAPGwa7gck7goaBh7rqG9tdRWuAt9rlpD3A\n2iRL2hvHa4E9bdvbSda0u4g2dh1LkrRAZv03kJN8G/gt4Lwk43TuCroTeCDJJuAV4IY2/VHgWmAM\neAe4GaCqjia5A3iyzbu9qibflL6Vzh1LZwKPtUWStIBmDYOqummGTVdPM7eALTMcZwewY5r6KHDR\nbH1IkuaPn0CWJBkGkiTDQJJED+8ZSDo5rNz6SN+e++Cd1/XtuTU3PDOQJBkGkiTDQJKEYSBJwjCQ\nJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShN9NJGkO9Ot7kfxOpLnjmYEkyTCQJBkGkiQMA0kShoEk\niQEKgyTrkryUZCzJ1n73I0mnkoEIgySnAd8A1gMXAjclubC/XUnSqWMgwgC4Ahirqper6l3gfmBD\nn3uSpFPGoHzobDlwqGt9HLhy6qQkm4HNbfWfk7y0AL316jzgJ/1uokf2Oj/sdX7M2Gu+tsCdHN/J\n8Gf672faMChh0JOq2g5s73cf00kyWlWr+91HL+x1ftjr/DhZej1Z+pzJoFwmOgys6FofajVJ0gIY\nlDB4EliV5IIkpwM3Arv73JMknTIG4jJRVb2X5EvAHuA0YEdVvdDntk7UQF6+moG9zg97nR8nS68n\nS5/TSlX1uwdJUp8NymUiSVIfGQaSJMPgw0iyI8mRJM931c5NMpLkQHtc0s8eJyVZkeTxJC8meSHJ\nl1t9oPpN8vEk30/yT63P/97qFyR5on1NyV+3GwwGQpLTkjyd5OG2PpC9JjmY5LkkzyQZbbWB+vlP\nSrI4yYNJfphkf5LPDGKvST7V/jwnl7eTfGUQe+2VYfDh3Aesm1LbCuytqlXA3rY+CN4D/qiqLgTW\nAFvaV30MWr8/A66qqouBS4B1SdYAXwPuqqpPAseATX3scaovA/u71ge519+uqku67oMftJ//pK8D\n362qTwMX0/nzHbheq+ql9ud5CXA58A7wHQaw155VlcuHWICVwPNd6y8B57fx+cBL/e5xhr4fAj4/\nyP0Cvwr8gM6n0H8CLGr1zwB7+t1f62WIzv/sVwEPAxngXg8C502pDdzPHzgH+DHtxpZB7nVKf2uB\n/3My9Hq8xTODubOsql5t49eAZf1sZjpJVgKXAk8wgP22yy7PAEeAEeBHwJtV9V6bMk7nq0sGwZ8D\nfwz8a1v/BIPbawF/n+Sp9pUuMIA/f+ACYAL4y3b57ZtJzmIwe+12I/DtNh70XmdkGMyD6vxaMFD3\n7Cb5NeBvga9U1dvd2wal36p6vzqn3UN0vrzw031uaVpJfgc4UlVP9buXHn2uqi6j863AW5L8ZvfG\nQfn50/nc02XAPVV1KfBTplxmGaBeAWjvC30B+Jup2wat19kYBnPn9STnA7THI33u5+eSfIxOEHyr\nqv6ulQe236p6E3iczqWWxUkmPxw5KF9T8lngC0kO0vmG3avoXOsexF6pqsPt8Qid69pXMJg//3Fg\nvKqeaOsP0gmHQex10nrgB1X1elsf5F6PyzCYO7uB4TYepnNtvu+SBLgX2F9Vf9a1aaD6TbI0yeI2\nPpPO+xr76YTCF9u0vvcJUFW3VdVQVa2kc4nge1X1ewxgr0nOSvLrk2M617efZ8B+/gBV9RpwKMmn\nWulq4EUGsNcuN/GLS0Qw2L0eX7/ftDgZFzo//FeB/0fnt5lNdK4Z7wUOAP8LOLfffbZeP0fnVPVZ\n4Jm2XDto/QL/EXi69fk88N9a/T8A3wfG6JyKn9HvP9Mpff8W8PCg9tp6+qe2vAD811YfqJ9/V7+X\nAKPtv4P/CSwZ4F7PAt4AzumqDWSvvSx+HYUkyctEkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJOD/\nA/OYYeD7u3i3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kdX9x5zgKj",
        "colab_type": "code",
        "outputId": "7e4fe892-0094-49ee-c54e-a5feeaa4ecca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max(length_vec_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n82yyoGxwors",
        "colab_type": "code",
        "outputId": "3a992563-698b-4ceb-f48d-1294ba9f94f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "num_classes = len(set(y_train))\n",
        "label2idx_dict = {}\n",
        "i = 0\n",
        "for label in set(y_train):\n",
        "  label2idx_dict[label] = i\n",
        "  i += 1\n",
        "\n",
        "label2idx_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Agriculture, food and drink': 5,\n",
              " 'Art and architecture': 1,\n",
              " 'Engineering and technology': 3,\n",
              " 'Geography and places': 0,\n",
              " 'History': 6,\n",
              " 'Language and literature': 15,\n",
              " 'Mathematics': 10,\n",
              " 'Media and drama': 11,\n",
              " 'Miscellaneous': 7,\n",
              " 'Music': 13,\n",
              " 'Natural sciences': 2,\n",
              " 'Philosophy and religion': 9,\n",
              " 'Social sciences and society': 12,\n",
              " 'Sports and recreation': 14,\n",
              " 'Video games': 4,\n",
              " 'Warfare': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuD_JITlt_3a",
        "colab_type": "code",
        "outputId": "ae8a1242-04af-4579-d96e-f17150dce7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def get_word2idx(dic, min_freq=3):\n",
        "  index_dic = {}\n",
        "  i = 0\n",
        "  index_dic['<pad>'] = i # 0 for pad\n",
        "  i += 1\n",
        "  index_dic['<unk>'] = i # 1 for unk\n",
        "  i += 1\n",
        "  for k,v in dic.items():\n",
        "    if v >= min_freq:\n",
        "      index_dic[k] = i\n",
        "      i += 1\n",
        "  return index_dic\n",
        "\n",
        "word2idx_train = get_word2idx(word_dict_train)\n",
        "print(\"length of indexed vocab\", len(word2idx_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of indexed vocab 61867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKP3b-IkvuXc",
        "colab_type": "code",
        "outputId": "eca19df1-4aa9-41c3-cb6d-caab2ac6bc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "text_size = 76\n",
        "n_features = 300\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "  # remove weird characters\n",
        "  s = re.sub(r\"[^A-Za-z0-9()!#$%&*+,./:;<=>?\\[\\\\\\]{|~}\\\"\\-\\'\\`]\", \" \", text)\n",
        "\n",
        "  sent = s.split()[:text_size]\n",
        "  ret = [0]*text_size\n",
        "  for i in range(len(sent)):\n",
        "    wd = sent[i]\n",
        "    offset = text_size - len(sent)\n",
        "    if wd in word2idx_train:\n",
        "      ret[i+offset] = word2idx_train[wd]\n",
        "    else:\n",
        "      ret[i+offset] = word2idx_train['<unk>']\n",
        "  return ret\n",
        "    \n",
        "\n",
        "def get_feature(data):\n",
        "  feat = []\n",
        "  for sent in data:\n",
        "    v = text_to_vector(sent)\n",
        "    feat.append(v)\n",
        "  return np.array(feat)\n",
        "\n",
        "feature_train = get_feature(x_train)\n",
        "feature_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,   36,   37,   38],\n",
              "       [   0,    0,    0, ...,   35,   55,   38],\n",
              "       [   0,    0,    0, ...,   77,    1,   38],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,   19,  933,   38],\n",
              "       [   0,    0,    0, ...,  199, 1635,   38],\n",
              "       [   0,    0,    0, ..., 2240, 1339,   38]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIrjZHX9dgtr",
        "colab_type": "code",
        "outputId": "f3f54030-21f6-477c-d0aa-58ec3d913b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def encode_label(label):\n",
        "  encode = []\n",
        "  for l in label:\n",
        "    if l == 'Media and darama': l = 'Media and drama'\n",
        "    encode.append(label2idx_dict[l])\n",
        "  return np.array(encode)\n",
        "\n",
        "y_train_encode = encode_label(y_train)\n",
        "y_train_encode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12, 14, 12, ...,  2,  1,  8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTgbMp6BOvDb",
        "colab_type": "code",
        "outputId": "f90f7367-150f-4c81-cb2e-6ad511710019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "valid_path = home_dir + 'topicclass/topicclass_valid.txt'\n",
        "x_valid, y_valid, _, length_vec_valid = load_data(valid_path)\n",
        "print(\"total valid set shape: \", len(x_valid))\n",
        "\n",
        "feature_valid = get_feature(x_valid)\n",
        "y_valid_encode = encode_label(y_valid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total valid set shape:  643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzy2P570TWyD",
        "colab_type": "code",
        "outputId": "502b63e7-d482-42ae-b898-fde42b561612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_path = home_dir + 'topicclass/topicclass_test.txt'\n",
        "x_test, _, _, _ = load_data(test_path)\n",
        "print(\"test set shape: \", len(x_test))\n",
        "\n",
        "feature_test = get_feature(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set shape:  697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lzC6z5A5wg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save and load embedding matrix\n",
        "def save(data, title):\n",
        "  pickle.dump(data, open( home_dir + title + \".pkl\", \"wb\" ))\n",
        "\n",
        "def load(title):\n",
        "  data = pickle.load(open( home_dir + title + \".pkl\", \"rb\" )) \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UyOYk41t_wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_embedding_matrix(word2idx_dict):\n",
        "    # randomize embedding\n",
        "    M = (np.random.rand(len(word2idx_dict), 300)-0.5)/2\n",
        "    pretrain_used = 0\n",
        "    with open(home_dir + 'cc.en.300.vec', 'r') as f:\n",
        "      total_pretrained_vec, dim = f.readline().split()\n",
        "      print(\"total pretrained vec:\", total_pretrained_vec, \"dim:\", dim)\n",
        "      for line in f:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx_dict:\n",
        "          M[word2idx_dict[word]] = np.array(tokens[1:])\n",
        "          pretrain_used += 1\n",
        "\n",
        "      print(\"%s out of %s (%f%%) words has pretrained embedding\" % (pretrain_used, len(word2idx_dict), pretrain_used/len(word2idx_dict)) )\n",
        "      return M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONwbJZ3y2YVn",
        "colab_type": "code",
        "outputId": "24baf32c-724b-4a2b-be77-372f2779df4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "embedding = get_word_embedding_matrix(word2idx_train)\n",
        "save(embedding, \"embedding_lower\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total pretrained vec: 2000000 dim: 300\n",
            "45098 out of 54083 (0.833866%) words has pretrained embedding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTmEM4cY6kNr",
        "colab_type": "code",
        "outputId": "c5a71c8d-9236-40c7-fad0-ea7ee8bbe2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding = load(\"embedding\")\n",
        "embedding.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61867, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I81CkoyRglW8",
        "colab_type": "text"
      },
      "source": [
        "# Model Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8qEl7wYgnS8",
        "colab_type": "text"
      },
      "source": [
        "### Cite: https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qheCHw5OdJBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# dataloaders\n",
        "train_data = TensorDataset(torch.from_numpy(feature_train), torch.from_numpy(y_train_encode))\n",
        "valid_data = TensorDataset(torch.from_numpy(feature_valid), torch.from_numpy(y_valid_encode))\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN7c030lRGkt",
        "colab_type": "code",
        "outputId": "82a696cc-fe20-42d2-9c91-1b68ca30a609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional=True, drop_prob=0.2):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            num_layers=n_layers, \n",
        "                            bidirectional=True, \n",
        "                            dropout=drop_prob, \n",
        "                            batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        # fc layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
        "\n",
        "        # sigmoid layer\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings: [batch size, sentence length, embedding dim]\n",
        "        embeds = self.embedding(x)\n",
        "        \n",
        "        # lstm\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        # pick the last word of each seq\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        \n",
        "        # dropout layer\n",
        "        out = self.dropout(lstm_out)\n",
        "\n",
        "        #  FC layer: [1, num_classes]\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # return sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "          # multipy by 2 since bidirectional\n",
        "            hidden = (weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "            \n",
        "        else:\n",
        "            hidden = (weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(word2idx_train)\n",
        "output_size = num_classes\n",
        "embedding_dim = n_features\n",
        "hidden_dim = 300\n",
        "n_layers = 2\n",
        "\n",
        "model = LSTMClassifier(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional=True)\n",
        "\n",
        "embedding = load(\"embedding\")\n",
        "embedding = torch.from_numpy(embedding)\n",
        "model.embedding.weight.data.copy_(embedding)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMClassifier(\n",
            "  (embedding): Embedding(61867, 300)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=600, out_features=16, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdObp1ckuFda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save(model, ep):\n",
        "  torch.save(model.state_dict(), home_dir+\"ass1_epoch\"+str(ep)+\".pt\")\n",
        "  \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T93NK8vMkY-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_on_gpu = True\n",
        "\n",
        "# loss and optimization functions\n",
        "lr = 0.0005\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "epochs = 10\n",
        "counter = 0\n",
        "print_every = 500\n",
        "clip = 5 # gradient clipping\n",
        "\n",
        "# # move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    model.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4mWt36tbfpJ",
        "colab_type": "code",
        "outputId": "e4485634-80be-4db5-e070-fbd3eadb6527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    acc_train = []\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history           \n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        if (train_on_gpu):\n",
        "          inputs = inputs.cuda()\n",
        "        \n",
        "        # get the output from the model\n",
        "        output, h = model(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # train acc\n",
        "        predict_train = torch.argmax(output, dim=-1)\n",
        "        acc_train.append(torch.sum(predict_train == labels).item()/batch_size)\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            acc = []\n",
        "            model.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "             \n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if (train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                inputs = inputs.type(torch.LongTensor)\n",
        "                if (train_on_gpu):\n",
        "                  inputs = inputs.cuda()\n",
        "                \n",
        "                output, val_h = model(inputs, val_h)\n",
        "                val_loss = criterion(output, labels)\n",
        "                \n",
        "                predict = torch.argmax(output, dim=-1)\n",
        "                acc.append(torch.sum(predict == labels).item()/batch_size)\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Train acc: {:.6f}\".format(np.mean(acc_train)),\n",
        "                  \"Val acc: {:.6f}\".format(np.mean(acc)))\n",
        "            \n",
        "    save(model, e)\n",
        "    print(\"saving epoch\", e)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 500... Loss: 2.613619... Val Loss: 2.567981 Train acc: 0.130875 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 1000... Loss: 2.484241... Val Loss: 2.551818 Train acc: 0.131719 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 1500... Loss: 2.671811... Val Loss: 2.557756 Train acc: 0.131667 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 2000... Loss: 2.598871... Val Loss: 2.556906 Train acc: 0.131703 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 2500... Loss: 2.551941... Val Loss: 2.533640 Train acc: 0.131637 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 3000... Loss: 2.718120... Val Loss: 2.549231 Train acc: 0.131656 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 3500... Loss: 2.446483... Val Loss: 2.555588 Train acc: 0.131321 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 4000... Loss: 2.671870... Val Loss: 2.553661 Train acc: 0.130953 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 4500... Loss: 2.489996... Val Loss: 2.536942 Train acc: 0.130868 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 5000... Loss: 2.570037... Val Loss: 2.564008 Train acc: 0.131275 Val acc: 0.104688\n",
            "Epoch: 1/10... Step: 5500... Loss: 2.549388... Val Loss: 2.450725 Train acc: 0.131784 Val acc: 0.142187\n",
            "Epoch: 1/10... Step: 6000... Loss: 2.500067... Val Loss: 2.375733 Train acc: 0.132995 Val acc: 0.146875\n",
            "Epoch: 1/10... Step: 6500... Loss: 2.399382... Val Loss: 2.339818 Train acc: 0.136745 Val acc: 0.173437\n",
            "Epoch: 1/10... Step: 7000... Loss: 2.346451... Val Loss: 2.296600 Train acc: 0.143143 Val acc: 0.209375\n",
            "Epoch: 1/10... Step: 7500... Loss: 2.169736... Val Loss: 2.239543 Train acc: 0.152213 Val acc: 0.287500\n",
            "saving epoch 0\n",
            "Epoch: 2/10... Step: 8000... Loss: 2.233786... Val Loss: 2.210223 Train acc: 0.274148 Val acc: 0.278125\n",
            "Epoch: 2/10... Step: 8500... Loss: 2.185556... Val Loss: 2.201275 Train acc: 0.294832 Val acc: 0.278125\n",
            "Epoch: 2/10... Step: 9000... Loss: 2.191868... Val Loss: 2.186732 Train acc: 0.286908 Val acc: 0.281250\n",
            "Epoch: 2/10... Step: 9500... Loss: 2.216484... Val Loss: 2.178103 Train acc: 0.285979 Val acc: 0.287500\n",
            "Epoch: 2/10... Step: 10000... Loss: 2.291858... Val Loss: 2.182591 Train acc: 0.287709 Val acc: 0.284375\n",
            "Epoch: 2/10... Step: 10500... Loss: 2.118437... Val Loss: 2.158198 Train acc: 0.294683 Val acc: 0.354687\n",
            "Epoch: 2/10... Step: 11000... Loss: 2.123101... Val Loss: 2.161378 Train acc: 0.302481 Val acc: 0.364063\n",
            "Epoch: 2/10... Step: 11500... Loss: 2.250102... Val Loss: 2.147380 Train acc: 0.308854 Val acc: 0.385937\n",
            "Epoch: 2/10... Step: 12000... Loss: 2.183110... Val Loss: 2.147088 Train acc: 0.314483 Val acc: 0.390625\n",
            "Epoch: 2/10... Step: 12500... Loss: 2.114085... Val Loss: 2.151011 Train acc: 0.318639 Val acc: 0.412500\n",
            "Epoch: 2/10... Step: 13000... Loss: 2.243209... Val Loss: 2.144785 Train acc: 0.323264 Val acc: 0.403125\n",
            "Epoch: 2/10... Step: 13500... Loss: 2.219146... Val Loss: 2.145621 Train acc: 0.328726 Val acc: 0.420312\n",
            "Epoch: 2/10... Step: 14000... Loss: 2.144439... Val Loss: 2.126177 Train acc: 0.332561 Val acc: 0.431250\n",
            "Epoch: 2/10... Step: 14500... Loss: 2.120083... Val Loss: 2.120977 Train acc: 0.337739 Val acc: 0.451562\n",
            "Epoch: 2/10... Step: 15000... Loss: 2.185967... Val Loss: 2.120637 Train acc: 0.344351 Val acc: 0.460938\n",
            "Epoch: 2/10... Step: 15500... Loss: 2.195458... Val Loss: 2.124137 Train acc: 0.349863 Val acc: 0.504687\n",
            "saving epoch 1\n",
            "Epoch: 3/10... Step: 16000... Loss: 2.077842... Val Loss: 2.112938 Train acc: 0.408617 Val acc: 0.510938\n",
            "Epoch: 3/10... Step: 16500... Loss: 2.124121... Val Loss: 2.112748 Train acc: 0.436116 Val acc: 0.529687\n",
            "Epoch: 3/10... Step: 17000... Loss: 2.100252... Val Loss: 2.105964 Train acc: 0.446362 Val acc: 0.451562\n",
            "Epoch: 3/10... Step: 17500... Loss: 2.176780... Val Loss: 2.104764 Train acc: 0.443053 Val acc: 0.498437\n",
            "Epoch: 3/10... Step: 18000... Loss: 2.095623... Val Loss: 2.110051 Train acc: 0.452011 Val acc: 0.532813\n",
            "Epoch: 3/10... Step: 18500... Loss: 2.080141... Val Loss: 2.081318 Train acc: 0.465651 Val acc: 0.542188\n",
            "Epoch: 3/10... Step: 19000... Loss: 2.040935... Val Loss: 2.079111 Train acc: 0.484375 Val acc: 0.643750\n",
            "Epoch: 3/10... Step: 19500... Loss: 2.017679... Val Loss: 2.069347 Train acc: 0.499002 Val acc: 0.589063\n",
            "Epoch: 3/10... Step: 20000... Loss: 2.097123... Val Loss: 2.057262 Train acc: 0.508879 Val acc: 0.596875\n",
            "Epoch: 3/10... Step: 20500... Loss: 2.171068... Val Loss: 2.069607 Train acc: 0.515497 Val acc: 0.592187\n",
            "Epoch: 3/10... Step: 21000... Loss: 2.165080... Val Loss: 2.062383 Train acc: 0.521708 Val acc: 0.654687\n",
            "Epoch: 3/10... Step: 21500... Loss: 2.018561... Val Loss: 2.036484 Train acc: 0.529608 Val acc: 0.640625\n",
            "Epoch: 3/10... Step: 22000... Loss: 2.200817... Val Loss: 2.039098 Train acc: 0.539791 Val acc: 0.681250\n",
            "Epoch: 3/10... Step: 22500... Loss: 2.041442... Val Loss: 2.031449 Train acc: 0.548972 Val acc: 0.682813\n",
            "Epoch: 3/10... Step: 23000... Loss: 2.026868... Val Loss: 2.047061 Train acc: 0.557422 Val acc: 0.726562\n",
            "Epoch: 3/10... Step: 23500... Loss: 1.984124... Val Loss: 2.032416 Train acc: 0.565948 Val acc: 0.712500\n",
            "saving epoch 2\n",
            "Epoch: 4/10... Step: 24000... Loss: 2.064442... Val Loss: 2.035755 Train acc: 0.718434 Val acc: 0.729688\n",
            "Epoch: 4/10... Step: 24500... Loss: 2.012638... Val Loss: 2.035956 Train acc: 0.730167 Val acc: 0.743750\n",
            "Epoch: 4/10... Step: 25000... Loss: 2.122860... Val Loss: 2.037782 Train acc: 0.732679 Val acc: 0.787500\n",
            "Epoch: 4/10... Step: 25500... Loss: 1.997281... Val Loss: 2.036958 Train acc: 0.731927 Val acc: 0.754687\n",
            "Epoch: 4/10... Step: 26000... Loss: 1.998863... Val Loss: 2.029547 Train acc: 0.736110 Val acc: 0.751563\n",
            "Epoch: 4/10... Step: 26500... Loss: 2.067817... Val Loss: 2.023935 Train acc: 0.739054 Val acc: 0.796875\n",
            "Epoch: 4/10... Step: 27000... Loss: 2.023218... Val Loss: 2.027007 Train acc: 0.742407 Val acc: 0.779687\n",
            "Epoch: 4/10... Step: 27500... Loss: 1.950325... Val Loss: 2.024584 Train acc: 0.743392 Val acc: 0.809375\n",
            "Epoch: 4/10... Step: 28000... Loss: 1.971102... Val Loss: 2.021592 Train acc: 0.742258 Val acc: 0.778125\n",
            "Epoch: 4/10... Step: 28500... Loss: 2.039640... Val Loss: 2.024577 Train acc: 0.742211 Val acc: 0.790625\n",
            "Epoch: 4/10... Step: 29000... Loss: 1.980519... Val Loss: 2.027710 Train acc: 0.743345 Val acc: 0.782813\n",
            "Epoch: 4/10... Step: 29500... Loss: 1.976575... Val Loss: 2.031505 Train acc: 0.745218 Val acc: 0.800000\n",
            "Epoch: 4/10... Step: 30000... Loss: 2.019474... Val Loss: 2.025148 Train acc: 0.747792 Val acc: 0.787500\n",
            "Epoch: 4/10... Step: 30500... Loss: 2.077109... Val Loss: 2.029267 Train acc: 0.750103 Val acc: 0.792188\n",
            "Epoch: 4/10... Step: 31000... Loss: 2.024020... Val Loss: 2.028732 Train acc: 0.749740 Val acc: 0.759375\n",
            "Epoch: 4/10... Step: 31500... Loss: 2.023595... Val Loss: 2.029252 Train acc: 0.749562 Val acc: 0.718750\n",
            "saving epoch 3\n",
            "Epoch: 5/10... Step: 32000... Loss: 1.953563... Val Loss: 2.027282 Train acc: 0.719934 Val acc: 0.790625\n",
            "Epoch: 5/10... Step: 32500... Loss: 2.049854... Val Loss: 2.029973 Train acc: 0.731103 Val acc: 0.803125\n",
            "Epoch: 5/10... Step: 33000... Loss: 2.066779... Val Loss: 2.034340 Train acc: 0.756725 Val acc: 0.781250\n",
            "Epoch: 5/10... Step: 33500... Loss: 1.955229... Val Loss: 2.033415 Train acc: 0.773526 Val acc: 0.790625\n",
            "Epoch: 5/10... Step: 34000... Loss: 2.019115... Val Loss: 2.037557 Train acc: 0.774818 Val acc: 0.790625\n",
            "Epoch: 5/10... Step: 34500... Loss: 2.005257... Val Loss: 2.033776 Train acc: 0.776716 Val acc: 0.801562\n",
            "Epoch: 5/10... Step: 35000... Loss: 2.049681... Val Loss: 2.038727 Train acc: 0.775764 Val acc: 0.796875\n",
            "Epoch: 5/10... Step: 35500... Loss: 2.017728... Val Loss: 2.042867 Train acc: 0.777796 Val acc: 0.770312\n",
            "Epoch: 5/10... Step: 36000... Loss: 1.951752... Val Loss: 2.037146 Train acc: 0.778890 Val acc: 0.785937\n",
            "Epoch: 5/10... Step: 36500... Loss: 1.967730... Val Loss: 2.034267 Train acc: 0.780797 Val acc: 0.785937\n",
            "Epoch: 5/10... Step: 37000... Loss: 1.987872... Val Loss: 2.036642 Train acc: 0.779754 Val acc: 0.785937\n",
            "Epoch: 5/10... Step: 37500... Loss: 1.998209... Val Loss: 2.031536 Train acc: 0.781125 Val acc: 0.810937\n",
            "Epoch: 5/10... Step: 38000... Loss: 1.973085... Val Loss: 2.031409 Train acc: 0.782123 Val acc: 0.735938\n",
            "Epoch: 5/10... Step: 38500... Loss: 1.959740... Val Loss: 2.036199 Train acc: 0.781014 Val acc: 0.793750\n",
            "Epoch: 5/10... Step: 39000... Loss: 1.957377... Val Loss: 2.032588 Train acc: 0.781228 Val acc: 0.776563\n",
            "Epoch: 5/10... Step: 39500... Loss: 2.011287... Val Loss: 2.042976 Train acc: 0.781089 Val acc: 0.785937\n",
            "saving epoch 4\n",
            "Epoch: 6/10... Step: 40000... Loss: 2.041920... Val Loss: 2.031341 Train acc: 0.834943 Val acc: 0.798438\n",
            "Epoch: 6/10... Step: 40500... Loss: 1.968269... Val Loss: 2.030659 Train acc: 0.836709 Val acc: 0.795312\n",
            "Epoch: 6/10... Step: 41000... Loss: 2.014883... Val Loss: 2.032694 Train acc: 0.837030 Val acc: 0.795312\n",
            "Epoch: 6/10... Step: 41500... Loss: 1.975999... Val Loss: 2.031205 Train acc: 0.836971 Val acc: 0.789062\n",
            "Epoch: 6/10... Step: 42000... Loss: 2.005858... Val Loss: 2.034892 Train acc: 0.836534 Val acc: 0.760938\n",
            "Epoch: 6/10... Step: 42500... Loss: 1.989070... Val Loss: 2.038372 Train acc: 0.832498 Val acc: 0.757812\n",
            "Epoch: 6/10... Step: 43000... Loss: 1.949010... Val Loss: 2.037409 Train acc: 0.829261 Val acc: 0.795312\n",
            "Epoch: 6/10... Step: 43500... Loss: 1.933255... Val Loss: 2.030210 Train acc: 0.828631 Val acc: 0.796875\n",
            "Epoch: 6/10... Step: 44000... Loss: 1.989538... Val Loss: 2.033165 Train acc: 0.829677 Val acc: 0.801562\n",
            "Epoch: 6/10... Step: 44500... Loss: 2.011981... Val Loss: 2.033113 Train acc: 0.829749 Val acc: 0.778125\n",
            "Epoch: 6/10... Step: 45000... Loss: 1.969340... Val Loss: 2.036098 Train acc: 0.828834 Val acc: 0.785937\n",
            "Epoch: 6/10... Step: 45500... Loss: 2.028352... Val Loss: 2.025844 Train acc: 0.829862 Val acc: 0.792188\n",
            "Epoch: 6/10... Step: 46000... Loss: 1.916445... Val Loss: 2.028458 Train acc: 0.830485 Val acc: 0.792188\n",
            "Epoch: 6/10... Step: 46500... Loss: 2.020887... Val Loss: 2.025819 Train acc: 0.831099 Val acc: 0.803125\n",
            "Epoch: 6/10... Step: 47000... Loss: 2.104292... Val Loss: 2.019947 Train acc: 0.832017 Val acc: 0.809375\n",
            "Epoch: 6/10... Step: 47500... Loss: 2.018152... Val Loss: 2.026773 Train acc: 0.832675 Val acc: 0.793750\n",
            "saving epoch 5\n",
            "Epoch: 7/10... Step: 48000... Loss: 1.971009... Val Loss: 2.032999 Train acc: 0.866635 Val acc: 0.801562\n",
            "Epoch: 7/10... Step: 48500... Loss: 1.951987... Val Loss: 2.026494 Train acc: 0.865723 Val acc: 0.809375\n",
            "Epoch: 7/10... Step: 49000... Loss: 1.902142... Val Loss: 2.022038 Train acc: 0.863069 Val acc: 0.820312\n",
            "Epoch: 7/10... Step: 49500... Loss: 1.926445... Val Loss: 2.021484 Train acc: 0.862919 Val acc: 0.815625\n",
            "Epoch: 7/10... Step: 50000... Loss: 1.922972... Val Loss: 2.032742 Train acc: 0.863431 Val acc: 0.785937\n",
            "Epoch: 7/10... Step: 50500... Loss: 1.909037... Val Loss: 2.027483 Train acc: 0.862634 Val acc: 0.792188\n",
            "Epoch: 7/10... Step: 51000... Loss: 2.000544... Val Loss: 2.034309 Train acc: 0.863277 Val acc: 0.795312\n",
            "Epoch: 7/10... Step: 51500... Loss: 1.957561... Val Loss: 2.033059 Train acc: 0.863233 Val acc: 0.795312\n",
            "Epoch: 7/10... Step: 52000... Loss: 1.921682... Val Loss: 2.027824 Train acc: 0.862837 Val acc: 0.798438\n",
            "Epoch: 7/10... Step: 52500... Loss: 1.897951... Val Loss: 2.018216 Train acc: 0.862458 Val acc: 0.815625\n",
            "Epoch: 7/10... Step: 53000... Loss: 1.955080... Val Loss: 2.020542 Train acc: 0.862549 Val acc: 0.807813\n",
            "Epoch: 7/10... Step: 53500... Loss: 1.952374... Val Loss: 2.027259 Train acc: 0.862412 Val acc: 0.809375\n",
            "Epoch: 7/10... Step: 54000... Loss: 2.005404... Val Loss: 2.027071 Train acc: 0.861994 Val acc: 0.801562\n",
            "Epoch: 7/10... Step: 54500... Loss: 2.003984... Val Loss: 2.025936 Train acc: 0.861872 Val acc: 0.804688\n",
            "Epoch: 7/10... Step: 55000... Loss: 2.057930... Val Loss: 2.029118 Train acc: 0.861859 Val acc: 0.792188\n",
            "Epoch: 7/10... Step: 55500... Loss: 1.925564... Val Loss: 2.029582 Train acc: 0.861583 Val acc: 0.807813\n",
            "saving epoch 6\n",
            "Epoch: 8/10... Step: 56000... Loss: 1.986172... Val Loss: 2.033097 Train acc: 0.873444 Val acc: 0.820312\n",
            "Epoch: 8/10... Step: 56500... Loss: 2.018659... Val Loss: 2.028922 Train acc: 0.874708 Val acc: 0.800000\n",
            "Epoch: 8/10... Step: 57000... Loss: 1.991687... Val Loss: 2.033725 Train acc: 0.874936 Val acc: 0.806250\n",
            "Epoch: 8/10... Step: 57500... Loss: 1.986099... Val Loss: 2.034888 Train acc: 0.876720 Val acc: 0.817187\n",
            "Epoch: 8/10... Step: 58000... Loss: 1.991107... Val Loss: 2.034080 Train acc: 0.875609 Val acc: 0.801562\n",
            "Epoch: 8/10... Step: 58500... Loss: 1.921806... Val Loss: 2.034426 Train acc: 0.875981 Val acc: 0.806250\n",
            "Epoch: 8/10... Step: 59000... Loss: 1.911985... Val Loss: 2.031895 Train acc: 0.875406 Val acc: 0.803125\n",
            "Epoch: 8/10... Step: 59500... Loss: 2.042505... Val Loss: 2.029799 Train acc: 0.874953 Val acc: 0.793750\n",
            "Epoch: 8/10... Step: 60000... Loss: 2.011043... Val Loss: 2.036057 Train acc: 0.874720 Val acc: 0.804688\n",
            "Epoch: 8/10... Step: 60500... Loss: 1.988323... Val Loss: 2.037165 Train acc: 0.874679 Val acc: 0.800000\n",
            "Epoch: 8/10... Step: 61000... Loss: 1.930633... Val Loss: 2.037896 Train acc: 0.874554 Val acc: 0.793750\n",
            "Epoch: 8/10... Step: 61500... Loss: 1.899483... Val Loss: 2.026664 Train acc: 0.874602 Val acc: 0.810937\n",
            "Epoch: 8/10... Step: 62000... Loss: 2.051742... Val Loss: 2.035380 Train acc: 0.874521 Val acc: 0.812500\n",
            "Epoch: 8/10... Step: 62500... Loss: 1.966613... Val Loss: 2.033971 Train acc: 0.874493 Val acc: 0.804688\n",
            "Epoch: 8/10... Step: 63000... Loss: 1.999013... Val Loss: 2.032854 Train acc: 0.874405 Val acc: 0.800000\n",
            "saving epoch 7\n",
            "Epoch: 9/10... Step: 63500... Loss: 1.959458... Val Loss: 2.033492 Train acc: 0.875000 Val acc: 0.800000\n",
            "Epoch: 9/10... Step: 64000... Loss: 1.967260... Val Loss: 2.035714 Train acc: 0.887251 Val acc: 0.796875\n",
            "Epoch: 9/10... Step: 64500... Loss: 1.928915... Val Loss: 2.043575 Train acc: 0.888163 Val acc: 0.789062\n",
            "Epoch: 9/10... Step: 65000... Loss: 2.053649... Val Loss: 2.041017 Train acc: 0.887005 Val acc: 0.790625\n",
            "Epoch: 9/10... Step: 65500... Loss: 1.923871... Val Loss: 2.033679 Train acc: 0.886603 Val acc: 0.785937\n",
            "Epoch: 9/10... Step: 66000... Loss: 1.921470... Val Loss: 2.040803 Train acc: 0.886101 Val acc: 0.784375\n",
            "Epoch: 9/10... Step: 66500... Loss: 1.971547... Val Loss: 2.045446 Train acc: 0.886022 Val acc: 0.784375\n",
            "Epoch: 9/10... Step: 67000... Loss: 1.882107... Val Loss: 2.037924 Train acc: 0.885080 Val acc: 0.803125\n",
            "Epoch: 9/10... Step: 67500... Loss: 1.944014... Val Loss: 2.035022 Train acc: 0.884558 Val acc: 0.793750\n",
            "Epoch: 9/10... Step: 68000... Loss: 1.916421... Val Loss: 2.037828 Train acc: 0.884324 Val acc: 0.790625\n",
            "Epoch: 9/10... Step: 68500... Loss: 1.927272... Val Loss: 2.035238 Train acc: 0.883558 Val acc: 0.803125\n",
            "Epoch: 9/10... Step: 69000... Loss: 2.010962... Val Loss: 2.033351 Train acc: 0.882965 Val acc: 0.801562\n",
            "Epoch: 9/10... Step: 69500... Loss: 1.945085... Val Loss: 2.034758 Train acc: 0.883009 Val acc: 0.787500\n",
            "Epoch: 9/10... Step: 70000... Loss: 1.923687... Val Loss: 2.032931 Train acc: 0.882937 Val acc: 0.789062\n",
            "Epoch: 9/10... Step: 70500... Loss: 1.898350... Val Loss: 2.037098 Train acc: 0.882910 Val acc: 0.789062\n",
            "Epoch: 9/10... Step: 71000... Loss: 1.925825... Val Loss: 2.034193 Train acc: 0.883099 Val acc: 0.779687\n",
            "saving epoch 8\n",
            "Epoch: 10/10... Step: 71500... Loss: 1.927316... Val Loss: 2.033156 Train acc: 0.899269 Val acc: 0.787500\n",
            "Epoch: 10/10... Step: 72000... Loss: 1.914744... Val Loss: 2.033374 Train acc: 0.897675 Val acc: 0.795312\n",
            "Epoch: 10/10... Step: 72500... Loss: 1.948877... Val Loss: 2.035934 Train acc: 0.895681 Val acc: 0.801562\n",
            "Epoch: 10/10... Step: 73000... Loss: 1.943263... Val Loss: 2.039457 Train acc: 0.893272 Val acc: 0.789062\n",
            "Epoch: 10/10... Step: 73500... Loss: 2.005057... Val Loss: 2.042390 Train acc: 0.892819 Val acc: 0.785937\n",
            "Epoch: 10/10... Step: 74000... Loss: 1.960310... Val Loss: 2.039472 Train acc: 0.892143 Val acc: 0.800000\n",
            "Epoch: 10/10... Step: 74500... Loss: 1.994469... Val Loss: 2.037933 Train acc: 0.892120 Val acc: 0.790625\n",
            "Epoch: 10/10... Step: 75000... Loss: 1.909056... Val Loss: 2.036818 Train acc: 0.891921 Val acc: 0.800000\n",
            "Epoch: 10/10... Step: 75500... Loss: 1.948218... Val Loss: 2.041904 Train acc: 0.891732 Val acc: 0.789062\n",
            "Epoch: 10/10... Step: 76000... Loss: 1.926851... Val Loss: 2.044638 Train acc: 0.891482 Val acc: 0.796875\n",
            "Epoch: 10/10... Step: 76500... Loss: 1.955337... Val Loss: 2.042403 Train acc: 0.891723 Val acc: 0.789062\n",
            "Epoch: 10/10... Step: 77000... Loss: 1.945568... Val Loss: 2.036169 Train acc: 0.891586 Val acc: 0.806250\n",
            "Epoch: 10/10... Step: 77500... Loss: 1.977185... Val Loss: 2.042642 Train acc: 0.891379 Val acc: 0.800000\n",
            "Epoch: 10/10... Step: 78000... Loss: 1.885143... Val Loss: 2.036129 Train acc: 0.891028 Val acc: 0.795312\n",
            "Epoch: 10/10... Step: 78500... Loss: 1.921892... Val Loss: 2.043931 Train acc: 0.890841 Val acc: 0.792188\n",
            "Epoch: 10/10... Step: 79000... Loss: 1.963711... Val Loss: 2.044427 Train acc: 0.890765 Val acc: 0.800000\n",
            "saving epoch 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzfYaCTWi-bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_load(name):\n",
        "  model =  model = LSTMClassifier(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional=True)\n",
        "  model.load_state_dict(torch.load(home_dir+name))\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BExUa7Za9QI5",
        "colab_type": "code",
        "outputId": "0ba768da-b8c9-43f1-b57e-0da9633951be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_on_gpu = True\n",
        "model = model_load(\"ass1_epoch9.pt\")\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0.0\n",
        "# acc = []\n",
        "# init hidden state\n",
        "h = model.init_hidden(batch_size)\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in valid_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    \n",
        "    inputs = inputs.type(torch.LongTensor)\n",
        "    if (train_on_gpu):\n",
        "       inputs = inputs.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = model(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output, labels)\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    predict = torch.argmax(output, dim=-1)\n",
        "    \n",
        "    num_correct += torch.sum(predict == labels)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(valid_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.037\n",
            "Test accuracy: 0.806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtgTO9iMzuKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}